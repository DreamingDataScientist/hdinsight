{"nbformat_minor": 2, "cells": [{"source": "# pyspark_ex(2) ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "sc", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1534132306360_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-helloc.am5bp12tnuiuda3gjpqeytesrb.ax.internal.cloudapp.net:8088/proxy/application_1534132306360_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-helloc.am5bp12tnuiuda3gjpqeytesrb.ax.internal.cloudapp.net:30060/node/containerlogs/container_1534132306360_0012_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n<SparkContext master=yarn appName=remotesparkmagics>"}], "metadata": {"collapsed": false}}, {"source": "## csv\ud30c\uc77c \ubd88\ub7ec\uc62c \ub584 sc.textFile\ub85c \uc624\uba74 RDD\ud615\ud0dc\ub85c \ub85c\ub4dc\ub41c\ub2e4.\n- printSchema()\uc640 show()\ud560 \ub54c \uc548\ub428", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "data = sc.textFile(\"/HdiSamples/HdiSamples/SensorSampleData/building/building.csv\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 6, "cell_type": "code", "source": "csv = sc.textFile(\"/HdiSamples/HdiSamples/SensorSampleData/building/building.csv\")\ncsv.printSchema()\ncsv.show(truncate=False)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "'RDD' object has no attribute 'printSchema'\nTraceback (most recent call last):\nAttributeError: 'RDD' object has no attribute 'printSchema'\n\n"}], "metadata": {"collapsed": false}}, {"source": "## csv\ud30c\uc77c \ubd88\ub7ec\uc62c \ub584 spark.read.csv()\ub85c \ud558\uba74 df\ub85c \ub85c\ub4dc\ub428.\n- printSchema()\uc640 show()\ud560 \ub54c \ub428", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "building_csv = spark.read.csv(\"/HdiSamples/HdiSamples/SensorSampleData/building/building.csv\", header=True, inferSchema=True)\nbuilding_csv.printSchema()\nbuilding_csv.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- BuildingID: integer (nullable = true)\n |-- BuildingMgr: string (nullable = true)\n |-- BuildingAge: integer (nullable = true)\n |-- HVACproduct: string (nullable = true)\n |-- Country: string (nullable = true)\n\n+----------+-----------+-----------+-----------+------------+\n|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n+----------+-----------+-----------+-----------+------------+\n|         1|         M1|         25|     AC1000|         USA|\n|         2|         M2|         27|     FN39TG|      France|\n|         3|         M3|         28|     JDNS77|      Brazil|\n|         4|         M4|         17|     GG1919|     Finland|\n|         5|         M5|          3|    ACMAX22|   Hong Kong|\n|         6|         M6|          9|     AC1000|   Singapore|\n|         7|         M7|         13|     FN39TG|South Africa|\n|         8|         M8|         25|     JDNS77|   Australia|\n|         9|         M9|         11|     GG1919|      Mexico|\n|        10|        M10|         23|    ACMAX22|       China|\n|        11|        M11|         14|     AC1000|     Belgium|\n|        12|        M12|         26|     FN39TG|     Finland|\n|        13|        M13|         25|     JDNS77|Saudi Arabia|\n|        14|        M14|         17|     GG1919|     Germany|\n|        15|        M15|         19|    ACMAX22|      Israel|\n|        16|        M16|         23|     AC1000|      Turkey|\n|        17|        M17|         11|     FN39TG|       Egypt|\n|        18|        M18|         25|     JDNS77|   Indonesia|\n|        19|        M19|         14|     GG1919|      Canada|\n|        20|        M20|         19|    ACMAX22|   Argentina|\n+----------+-----------+-----------+-----------+------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "building_csv", "outputs": [{"output_type": "stream", "name": "stdout", "text": "DataFrame[BuildingID: int, BuildingMgr: string, BuildingAge: int, HVACproduct: string, Country: string]"}], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "b_data = building_csv.select(\"BuildingID\", \"BuildingAge\", \"HVACproduct\")\nb_data.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-----------+-----------+\n|BuildingID|BuildingAge|HVACproduct|\n+----------+-----------+-----------+\n|         1|         25|     AC1000|\n|         2|         27|     FN39TG|\n|         3|         28|     JDNS77|\n|         4|         17|     GG1919|\n|         5|          3|    ACMAX22|\n|         6|          9|     AC1000|\n|         7|         13|     FN39TG|\n|         8|         25|     JDNS77|\n|         9|         11|     GG1919|\n|        10|         23|    ACMAX22|\n|        11|         14|     AC1000|\n|        12|         26|     FN39TG|\n|        13|         25|     JDNS77|\n|        14|         17|     GG1919|\n|        15|         19|    ACMAX22|\n|        16|         23|     AC1000|\n|        17|         11|     FN39TG|\n|        18|         25|     JDNS77|\n|        19|         14|     GG1919|\n|        20|         19|    ACMAX22|\n+----------+-----------+-----------+"}], "metadata": {"collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "from pyspark.sql.types import *\nfrom pyspark.sql.functions import *", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## HVAC\ub370\uc774\ud130 \uc2a4\ud0a4\ub9c8 \uc9c0\uc815\ud558\uae30", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "# load HVAC.csv\nschma = StructType([\n        StructField(\"Date\", StringType(), False),\n        StructField(\"Time\", StringType(), False),\n        StructField(\"TargetTemp\", IntegerType(), False),\n        StructField(\"ActualTemp\", StringType(), False),\n        StructField(\"System\", IntegerType(), False),\n        StructField(\"SystemAge\", IntegerType(), False),\n        StructField(\"BuildingID\", IntegerType(), False),\n        \n    ])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## HVAC\ub370\uc774\ud130 \ub85c\ub4dc\ud558\uace0 \ub9cc\ub4e4\uc5b4\uc900 schema \uc801\uc6a9", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "hvac_csv = spark.read.csv(\"/HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv\", schema=schma, header=True)\nhvac_csv.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Date: string (nullable = true)\n |-- Time: string (nullable = true)\n |-- TargetTemp: integer (nullable = true)\n |-- ActualTemp: string (nullable = true)\n |-- System: integer (nullable = true)\n |-- SystemAge: integer (nullable = true)\n |-- BuildingID: integer (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 32, "cell_type": "code", "source": "hvac_csv.select('Date', 'Time', 'TargetTemp', 'ActualTemp', 'System', 'SystemAge', 'BuildingID').show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+--------+----------+----------+------+---------+----------+\n|   Date|    Time|TargetTemp|ActualTemp|System|SystemAge|BuildingID|\n+-------+--------+----------+----------+------+---------+----------+\n| 6/1/13| 0:00:01|        66|        58|    13|       20|         4|\n| 6/2/13| 1:00:01|        69|        68|     3|       20|        17|\n| 6/3/13| 2:00:01|        70|        73|    17|       20|        18|\n| 6/4/13| 3:00:01|        67|        63|     2|       23|        15|\n| 6/5/13| 4:00:01|        68|        74|    16|        9|         3|\n| 6/6/13| 5:00:01|        67|        56|    13|       28|         4|\n| 6/7/13| 6:00:01|        70|        58|    12|       24|         2|\n| 6/8/13| 7:00:01|        70|        73|    20|       26|        16|\n| 6/9/13| 8:00:01|        66|        69|    16|        9|         9|\n|6/10/13| 9:00:01|        65|        57|     6|        5|        12|\n|6/11/13|10:00:01|        67|        70|    10|       17|        15|\n|6/12/13|11:00:01|        69|        62|     2|       11|         7|\n|6/13/13|12:00:01|        69|        73|    14|        2|        15|\n|6/14/13|13:00:01|        65|        61|     3|        2|         6|\n|6/15/13|14:00:01|        67|        59|    19|       22|        20|\n|6/16/13|15:00:01|        65|        56|    19|       11|         8|\n|6/17/13|16:00:01|        67|        57|    15|        7|         6|\n|6/18/13|17:00:01|        66|        57|    12|        5|        13|\n|6/19/13|18:00:01|        69|        58|     8|       22|         4|\n|6/20/13|19:00:01|        67|        55|    17|        5|         7|\n+-------+--------+----------+----------+------+---------+----------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "## hvac_data\ub97c .select()\ub85c \uc2e4\uc81c\uc628\ub3c4\uac00 \ub354 \ub192\uc558\ub358 \ube4c\ub529\uc744 \ucd94\ucd9c", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "hvac_data = hvac_csv.select(\"BuildingID\", \"ActualTemp\", \"TargetTemp\").filter(col(\"ActualTemp\") > col(\"TargetTemp\"))\nhvac_data.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+----------+----------+\n|BuildingID|ActualTemp|TargetTemp|\n+----------+----------+----------+\n|        18|        73|        70|\n|         3|        74|        68|\n|        16|        73|        70|\n|         9|        69|        66|\n|        15|        70|        67|\n|        15|        73|        69|\n|        17|        72|        69|\n|         9|        69|        66|\n|        18|        71|        70|\n|         8|        71|        67|\n|        13|        77|        66|\n|         9|        69|        67|\n|        10|        78|        70|\n|        19|        72|        67|\n|         2|        73|        67|\n|        10|        75|        65|\n|         2|        68|        67|\n|         6|        77|        65|\n|         3|        67|        66|\n|         1|        68|        67|\n+----------+----------+----------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "## \uc2e4\uc81c \uc628\ub3c4\uac00 \ub354 \ub192\uc740 \ube4c\ub529\uc744 hot_building\uc774\ub77c\ub294 \ubcc0\uc218\uc5d0  building_data\uc640 join\ud558\uc5ec \ub123\uae30", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "# join\nhot_buildings = building_data.join(hvac_data, \"BuildingID\")\nhot_buildings.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-----------+-----------+----------+----------+\n|BuildingID|BuildingAge|HVACproduct|ActualTemp|TargetTemp|\n+----------+-----------+-----------+----------+----------+\n|        18|         25|     JDNS77|        73|        70|\n|         3|         28|     JDNS77|        74|        68|\n|        16|         23|     AC1000|        73|        70|\n|         9|         11|     GG1919|        69|        66|\n|        15|         19|    ACMAX22|        70|        67|\n|        15|         19|    ACMAX22|        73|        69|\n|        17|         11|     FN39TG|        72|        69|\n|         9|         11|     GG1919|        69|        66|\n|        18|         25|     JDNS77|        71|        70|\n|         8|         25|     JDNS77|        71|        67|\n|        13|         25|     JDNS77|        77|        66|\n|         9|         11|     GG1919|        69|        67|\n|        10|         23|    ACMAX22|        78|        70|\n|        19|         14|     GG1919|        72|        67|\n|         2|         27|     FN39TG|        73|        67|\n|        10|         23|    ACMAX22|        75|        65|\n|         2|         27|     FN39TG|        68|        67|\n|         6|          9|     AC1000|        77|        65|\n|         3|         28|     JDNS77|        67|        66|\n|         1|         25|     AC1000|        68|        67|\n+----------+-----------+-----------+----------+----------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "## hot_buildings \ubcc0\uc218\ub97c createOrReplaceTempView\ub85c \ubcc0\ud658\ud558\uc5ec \ucd9c\ub825", "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": "# spark SQL\nhot_buildings.createOrReplaceTempView(\"tmpHotBuildings\")\nspark.sql(\"SELECT HVACproduct, COUNT(*) AS installations FROM tmpHotBuildings GROUP BY HVACproduct\").show()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+-------------+\n|HVACproduct|installations|\n+-----------+-------------+\n|    ACMAX22|          790|\n|     AC1000|          773|\n|     JDNS77|          778|\n|     FN39TG|          833|\n|     GG1919|          740|\n+-----------+-------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "\n# save table\nbuilding_csv.write.saveAsTable(\"building\")\nbuilding_df = spark.sql(\"SELECT * FROM building\")\nbuilding_df.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-----------+-----------+-----------+------------+\n|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n+----------+-----------+-----------+-----------+------------+\n|         1|         M1|         25|     AC1000|         USA|\n|         2|         M2|         27|     FN39TG|      France|\n|         3|         M3|         28|     JDNS77|      Brazil|\n|         4|         M4|         17|     GG1919|     Finland|\n|         5|         M5|          3|    ACMAX22|   Hong Kong|\n|         6|         M6|          9|     AC1000|   Singapore|\n|         7|         M7|         13|     FN39TG|South Africa|\n|         8|         M8|         25|     JDNS77|   Australia|\n|         9|         M9|         11|     GG1919|      Mexico|\n|        10|        M10|         23|    ACMAX22|       China|\n|        11|        M11|         14|     AC1000|     Belgium|\n|        12|        M12|         26|     FN39TG|     Finland|\n|        13|        M13|         25|     JDNS77|Saudi Arabia|\n|        14|        M14|         17|     GG1919|     Germany|\n|        15|        M15|         19|    ACMAX22|      Israel|\n|        16|        M16|         23|     AC1000|      Turkey|\n|        17|        M17|         11|     FN39TG|       Egypt|\n|        18|        M18|         25|     JDNS77|   Indonesia|\n|        19|        M19|         14|     GG1919|      Canada|\n|        20|        M20|         19|    ACMAX22|   Argentina|\n+----------+-----------+-----------+-----------+------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 36, "cell_type": "code", "source": "# Query Hive Table\nspark.sql(\"SHOW TABLES\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+---------------+-----------+\n|database|      tableName|isTemporary|\n+--------+---------------+-----------+\n| default|       building|      false|\n| default|hivesampletable|      false|\n| default|           hvac|      false|\n|        |tmphotbuildings|       true|\n+--------+---------------+-----------+"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}